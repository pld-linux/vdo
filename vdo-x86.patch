--- vdo-8.3.1.1/utils/uds/linux/atomic.h.orig	2025-03-14 01:17:27.000000000 +0100
+++ vdo-8.3.1.1/utils/uds/linux/atomic.h	2025-03-24 21:04:02.195782628 +0100
@@ -79,7 +79,7 @@ static inline void barrier(void)
  **/
 static inline void smp_mb(void)
 {
-#if defined __x86_64__
+#if defined __x86_64__ || defined __i386__
   /*
    * X86 full fence. Supposedly __sync_synchronize() will do this, but
    * either the GCC documentation is a lie or GCC is broken.
@@ -114,7 +114,7 @@ static inline void smp_mb(void)
  **/
 static inline void smp_rmb(void)
 {
-#if defined __x86_64__
+#if defined __x86_64__ || defined __i386__
   // The implementation on x86 is more aggressive than necessary.
   __asm__ __volatile__("lfence" : : : "memory");
 #elif defined __aarch64__
@@ -143,7 +143,7 @@ static inline void smp_rmb(void)
  **/
 static inline void smp_wmb(void)
 {
-#if defined __x86_64__
+#if defined __x86_64__ || defined __i386__
   // The implementation on x86 is more aggressive than necessary.
   __asm__ __volatile__("sfence" : : : "memory");
 #elif defined __aarch64__
@@ -167,7 +167,7 @@ static inline void smp_wmb(void)
  **/
 static inline void smp_mb__before_atomic(void)
 {
-#if defined(__x86_64__) || defined(__s390__)
+#if defined(__x86_64__) || defined(__s390__) || defined __i386__
   // Atomic operations are already serializing on x86 and s390
   barrier();
 #else
@@ -181,7 +181,7 @@ static inline void smp_mb__before_atomic
  **/
 static inline void smp_mb__after_atomic(void)
 {
-#if defined(__x86_64__) || defined(__s390__)
+#if defined(__x86_64__) || defined(__s390__) || defined __i386__
   // Atomic operations are already serializing on x86 and s390
   barrier();
 #else
--- vdo-8.3.1.1/utils/uds/linux/cache.h.orig	2025-03-14 01:17:27.000000000 +0100
+++ vdo-8.3.1.1/utils/uds/linux/cache.h	2025-03-24 20:43:32.302445532 +0100
@@ -15,7 +15,7 @@
 #define L1_CACHE_BYTES 128
 #elif defined(__s390x__)
 #define L1_CACHE_BYTES 256
-#elif defined(__x86_64__) || defined(__aarch64__) || defined(__riscv) || defined (__loongarch64)
+#elif defined(__x86_64__) || defined(__aarch64__) || defined(__riscv) || defined (__loongarch64) || defined(__i386__)
 #define L1_CACHE_BYTES 64
 #else
 #error "unknown cache line size"
